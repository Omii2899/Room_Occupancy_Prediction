{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VoWNopnlwy5p",
    "outputId": "523b559f-9adc-46d0-c7b5-f2267e2405a2",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-20T02:46:47.294698Z"
    }
   },
   "source": [
    "!pip install ucimlrepo\n",
    "!pip install imblearn"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Users/omii/anaconda3/envs/Lab_1_7300/lib/python3.10/site-packages (0.0.6)\r\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaJFfJLnL7G8"
   },
   "source": [
    "### Installing  Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WcXm00zWXYDR",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "from google.colab import files\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZL7a_WfVzCO"
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9J7YzigyuOTc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "outputId": "0be33835-be53-44e2-ccfd-efa1db93b15b",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "room_occupancy_estimation = fetch_ucirepo(id=864)\n",
    "room_occupancy_df = pd.concat([room_occupancy_estimation.data.features, room_occupancy_estimation.data.targets], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8vwBpu7WEgI"
   },
   "source": [
    "### EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yGwxU0jJzRem",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hzm3mxszYhQT",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z5wJhOlfzv3H",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xq6jOI56z0W6",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eiAPwG1DYhN3",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XQHdq9bw0OKn",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# checking for null values\n",
    "\n",
    "def find_null_percent(df):\n",
    "\n",
    "    # creating a data frame with percent of null values in each column\n",
    "    null_percent = (df.isnull().mean()*100).reset_index().rename(columns = {'index': 'column_name', 0:'percent_of_null_values'})\n",
    "    return null_percent\n",
    "\n",
    "find_null_percent(room_occupancy_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t6B63HWlMsbh",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JZRr-NqfMsYR",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df[\"Date\"] = pd.to_datetime(room_occupancy_df[\"Date\"])\n",
    "room_occupancy_df[\"Time\"] = pd.to_timedelta(room_occupancy_df[\"Time\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "po9cGZxjMsVo",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q0h5gMbwMsTT",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df['Datetime'] = room_occupancy_df[\"Date\"] + room_occupancy_df[\"Time\"]\n",
    "# Extracting hour of the day\n",
    "room_occupancy_df['Hour_of_day'] = room_occupancy_df['Datetime'].dt.hour\n",
    "# Extracting day of the week (Monday= 0 and Sunday=6)\n",
    "room_occupancy_df['Day_of_week'] = room_occupancy_df['Datetime'].dt.dayofweek"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iAkgdhKPMsQ3",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "pivot_table = room_occupancy_df.pivot_table(index='Hour_of_day', columns='Room_Occupancy_Count', aggfunc='size', fill_value=0)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    col = pivot_table.columns[i]\n",
    "    ax.plot(pivot_table.index, pivot_table[col], marker='o')\n",
    "    ax.set_title(f'Occupancy {col}')\n",
    "    ax.set_xlabel('Hour of the Day')\n",
    "    ax.set_ylabel('Count of Records')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "saVeVfk8MsOQ",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Group by date and count the number of records for each day\n",
    "daily_counts = room_occupancy_df.groupby('Date').size().reset_index(name='count')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(daily_counts['Date'], daily_counts['count'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Number of Records per Day')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Records')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ae7ImQJmMsLy",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Function to categorize time of day\n",
    "def categorize_time_of_day(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Night'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Evening'\n",
    "\n",
    "# Extracting hour from datetime column and apply categorize_time_of_day function\n",
    "room_occupancy_df['time_of_day'] = room_occupancy_df['Datetime'].dt.hour.apply(categorize_time_of_day)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "epsb0JmrMsI5",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "#Classification of records based on time of day\n",
    "plt.figure(figsize=(12,7))\n",
    "ax = sns.countplot(data = room_occupancy_df,x = 'Room_Occupancy_Count', hue = 'time_of_day', hue_order = ['Morning','Afternoon','Evening','Night'])\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bwumCfhY6Po"
   },
   "source": [
    "## Reasons for dropping 'Date' and 'Time' column's\n",
    "\n",
    "- As we have seen in the graph that there 0 data values for class 0 during Afternoon, Evening, Morning and alot of values for Night only. For our learning purposes we don't want that to effect of hour of day to skew the results.\n",
    "- Also, as we can see that hour of day, day, month, year(features extracted from date and time) have no impact on  data. We would prefer dropping 'Date' and 'Time'\n",
    "-Since, number of records on different days is highly varying we don’t want it to affect our learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QE8_AVi-NTvk",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df= room_occupancy_df.drop(columns=['Date', 'Time', 'Datetime', 'Hour_of_day', 'Day_of_week','time_of_day'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vUr2xckEDj12",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# checking for duplicate rows\n",
    "room_occupancy_df.drop_duplicates(inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zGgUCX_kO1i4",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "drrZ2NpCNTtM",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-aFPVGCH2kHS",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "numerical_columns = ['S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp','S1_Light', 'S2_Light', 'S3_Light', 'S4_Light', 'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound',\n",
    "                     'S5_CO2', 'S5_CO2_Slope']\n",
    "# Checking correlation between features\n",
    "corr_matrix = room_occupancy_df[numerical_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt=\".1f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XCSTAYTetGtJ",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Dropping features with correlation greater than 0.9\n",
    "room_occupancy_df = room_occupancy_df.drop([ 'S3_Temp', 'S4_Temp','S5_CO2'],axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nPxgJHfJFb-2",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m5tAlCRhPa_0",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9pUJTlS8yqF"
   },
   "source": [
    "###Checking for Outliers-"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xz75-L_O3KYH",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "numerical_columns = ['S1_Temp', 'S2_Temp', 'S1_Light', 'S2_Light', 'S3_Light', 'S4_Light', 'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound', 'S5_CO2_Slope']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kRSdrsgu68uA",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Using box-plot\n",
    "n_cols = 4\n",
    "n_rows = 3\n",
    "\n",
    "# Create a figure with subplots in a 4x4 grid\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 12))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through all columns and create a box plot in each subplot\n",
    "for i, column in enumerate(numerical_columns):\n",
    "    if i < n_cols * n_rows:  # Check if the current index is within the range of available subplots\n",
    "        room_occupancy_df.boxplot(column=[column], vert=False, ax=axes[i])  # Create box plot\n",
    "        axes[i].set_title(f\"Box plot for {column}\")  # Set title to the name of the column\n",
    "    else:\n",
    "        break  # Exit the loop if there are more columns than subplots available\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q71882FCGXwj",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Determine the number of rows and columns for the subplot grid\n",
    "n_cols = 4\n",
    "n_rows = 2\n",
    "\n",
    "# Create a figure with subplots in a 4x4 grid\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 8))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through all columns and create a box plot in each subplot\n",
    "for i, column in enumerate(['S1_Light', 'S2_Light', 'S3_Light', 'S4_Light']):\n",
    "    sns.boxplot(data= (room_occupancy_df[column]), ax = axes[i], orient = 'h')  # Create box plot\n",
    "    axes[i].set_title(f\"Box plot for {column}\")  # Set title to the name of the column\n",
    "\n",
    "# Loop through all columns and create a box plot in each subplot\n",
    "for i, column in enumerate(['S1_Light', 'S2_Light', 'S3_Light', 'S4_Light']):\n",
    "    sns.boxplot(data= np.log(room_occupancy_df[column]+1), ax = axes[i+4], orient = 'h')  # Create box plot\n",
    "    axes[i+4].set_title(f\"Box plot for Log Transform of {column}\")  # Set title to the name of the column\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVkOuHv8-3xA"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CUcfyq2utGpv",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "light_features = ['S1_Light', 'S2_Light', 'S3_Light', 'S4_Light']\n",
    "\n",
    "# Applying Box-Cox transformation\n",
    "for feature in light_features:\n",
    "    log_values = np.log(room_occupancy_df[feature]+1)\n",
    "    room_occupancy_df[f'{feature}_log'] = log_values\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dEu3qVlxQcWv",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtwSSWGh3UU8"
   },
   "source": [
    "### Checking if features follow a Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kHoWTFNbtGip",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "features = ['S1_Temp', 'S2_Temp', 'S1_Light', 'S2_Light', 'S3_Light', 'S4_Light',\n",
    "            'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound', 'S5_CO2_Slope']\n",
    "\n",
    "# Generate density plots for each feature\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i]\n",
    "    sns.kdeplot(room_occupancy_df[feature], ax=ax)\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaO-oVDRwNvO"
   },
   "source": [
    "All sound feature have left skewness but still follow a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LVnBGoFDtGcf",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Flatten axes if necessary\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generate Q-Q plots for each feature\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i]\n",
    "    stats.probplot(room_occupancy_df[feature], dist=\"norm\", plot=ax)\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel('Theoretical quantiles')\n",
    "    ax.set_ylabel('Ordered Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhoq2P7FxDKM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jw3fdrj6H5Rf",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from scipy.stats import boxcox\n",
    "sound_features = ['S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound']\n",
    "\n",
    "# Applying Box-Cox transformation\n",
    "for feature in sound_features:\n",
    "    box_cox_data, lambda_value = boxcox(room_occupancy_df[feature] + 1)\n",
    "    room_occupancy_df[f'{feature}_boxcox'] = box_cox_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qL0qquu6yGYT",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "n_cols = 4\n",
    "n_rows = 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(15, 8))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through all columns and create a density plot in each subplot\n",
    "for i, column in enumerate(sound_features):\n",
    "    sns.kdeplot(room_occupancy_df[column], ax=axes[i], color='blue')\n",
    "    axes[i].set_title(f\" {column}\")\n",
    "\n",
    "# Loop through all columns and create a density plot of the Box-Cox transformed features in each subplot\n",
    "for i, column in enumerate(sound_features):\n",
    "    sns.kdeplot(room_occupancy_df[f'{column}_boxcox'], ax=axes[i+4])\n",
    "    axes[i+4].set_title(f\"{column}\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNdyaqFVK69L"
   },
   "source": [
    "\n",
    "\n",
    "*   After different transformations 'S2_Temp' still had many outliers, so we are dropping it. It also had high corelation with 'S1_Temp'.\n",
    "*   'S1_Sound_boxcox', 'S2_Sound_boxcox', 'S3_Sound_boxcox' follow Gaussian distribution but 'S4_Sound_boxcox' still does not. So we will drop S4_Sound feature.\n",
    "\n",
    "*   We are also dropping 'S1_Light', 'S2_Light', 'S3_Light', 'S4_Light' as we created their transformed form using log transformation, which removed outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u1wXU_NkfIZS",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df= room_occupancy_df.drop(['S2_Temp','S1_Light', 'S2_Light', 'S3_Light', 'S4_Light',  'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound'], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HGmTGa3SzJKB",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "room_occupancy_df.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfym2ZAOTsnu"
   },
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VbGGlqsufIVN",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "X = room_occupancy_df.drop(['Room_Occupancy_Count'], axis=1)\n",
    "y = room_occupancy_df['Room_Occupancy_Count']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dlO0ntRYWrAv",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "X.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbNUEvg2UnXg"
   },
   "source": [
    "### SMOTE on train data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "brfFi5TZUqta",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def oversample_minority_classes(X, y):\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "X_resampled, y_resampled = oversample_minority_classes(X, y)\n",
    "\n",
    "print(X_resampled.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e62RUTtMXlza"
   },
   "source": [
    "### Split into test and training data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "die0039hXpUe",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X_resampled, y_resampled, test_size = 0.20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        matrix[int(true), int(pred)] += 1\n",
    "    return matrix\n",
    "\n",
    "def precision_recall_accuracy(conf_matrix):\n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "    for i in range(num_classes):\n",
    "        true_positives = conf_matrix[i, i]\n",
    "        false_positives = np.sum(conf_matrix[:, i]) - true_positives\n",
    "        false_negatives = np.sum(conf_matrix[i, :]) - true_positives\n",
    "        precision[i] = true_positives / (true_positives + false_positives)\n",
    "        recall[i] = true_positives / (true_positives + false_negatives)\n",
    "    return precision, recall, accuracy"
   ],
   "metadata": {
    "id": "1_-66mPwYLCt",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pEVPnnsT7HX"
   },
   "source": [
    "# Random Classifier - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XC0QU5sAfIRc",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "class BaselineClassifier():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self):\n",
    "        return np.random.randint(0, len(np.unique(self.y)) , size=len(self.X))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vdE-3oFMfIN5",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "baseline = BaselineClassifier(X_resampled, y_resampled)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gf2D4WvuXJQD",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "baseline_ = baseline.predict()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "conf_matrix = confusion_matrix(testY, baseline_, 4)\n",
    "precision, recall, accuracy = precision_recall_accuracy(conf_matrix)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "id": "xv9QBS8eSd80",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVReuat3UAHh"
   },
   "source": [
    "# Naive Baiyes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rujhQec2Uiah",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Naive Bayes Classifier\n",
    "        \"\"\"\n",
    "        self.classes = None\n",
    "        self.params = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the Naive Bayes model to the training data by calculating the mean, variance,\n",
    "        and prior probabilities for each class.\n",
    "\n",
    "        :param X: the feature data used for training.\n",
    "        :param y: the target labels corresponding to the rows in X.\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        total = len(y)\n",
    "        self.params = {cls: self._estimate_parameters(X[y == cls], total) for cls in self.classes}\n",
    "\n",
    "    def _estimate_parameters(self, data, total):\n",
    "        \"\"\"\n",
    "        Estimates the mean, variance, and prior probability function\n",
    "\n",
    "        :param data: data subset for a particular class.\n",
    "        :return: dict which is a dictionary containing the mean, variance adjusted by a small constant\n",
    "                (in order to avoid division by zero, and prior probability)\n",
    "        \"\"\"\n",
    "        small_constant = 1e-6\n",
    "        laplace_smoothing = 1\n",
    "        k = len(self.classes)\n",
    "        return {\n",
    "            'mean': data.mean(),\n",
    "            'var': data.var() + small_constant,\n",
    "            'prior': (len(data) + laplace_smoothing) / (total + k)\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels function\n",
    "\n",
    "        :param X: pandas.DataFrame - The feature data for prediction.\n",
    "        :return: predicted class labels\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_single(row) for index, row in X.iterrows()])\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Single data point and returns the class with the highest posterior probability\n",
    "\n",
    "        :param x: a single data point\n",
    "        :return: the class label with the highest posterior probability\n",
    "        \"\"\"\n",
    "        posteriors = [self._compute_posterior(cls, x) for cls in self.classes]\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _compute_posterior(self, cls, x):\n",
    "        \"\"\"\n",
    "        Computes the posterior probability function\n",
    "\n",
    "        :param cls: the class label\n",
    "        :param x: the data point features\n",
    "        :return: a float value which is log of the posterior probability\n",
    "        \"\"\"\n",
    "        prior = self.params[cls]['prior']\n",
    "        log_prior = np.log(max(prior, 1e-10))\n",
    "        conditional = np.sum(np.log(self.prob_density_function(cls, x) + 1e-10))\n",
    "        return log_prior + conditional\n",
    "\n",
    "    def prob_density_function(self, cls, x):\n",
    "        \"\"\"\n",
    "        Calculates the probability density function\n",
    "\n",
    "        :param cls: the class label\n",
    "        :param x: feature values\n",
    "        :return: a float value which is the computed probability density\n",
    "        \"\"\"\n",
    "        mean = self.params[cls]['mean']\n",
    "        var = self.params[cls]['var']\n",
    "        numerator = np.exp(- (x-mean)**2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WMOtrUWIfIKl",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "naive_bayes = NaiveBayesClassifier()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CAfugOnbfLKQ",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "naive_bayes.fit(trainX, trainY)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = naive_bayes.predict(testX)"
   ],
   "metadata": {
    "id": "9h6cwi_kWM7Z",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "conf_matrix = confusion_matrix(testY, predictions, 4)\n",
    "precision, recall, accuracy = precision_recall_accuracy(conf_matrix)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "id": "JvSd574cJBpR",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfl7N6HnVOQo"
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X_resampled, y_resampled, test_size = 0.20)"
   ],
   "metadata": {
    "id": "FIDcNAmaHwTw",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XTft8u6WfLCg",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Support Vector machine class using gradient descent\n",
    "\n",
    "class SVM:\n",
    "\n",
    "    def __init__(self, learning_rate = 0.00001, lambda_ = 0.001, max_iterations = 100, normalize='True'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_ = lambda_\n",
    "        self.max_iterations = max_iterations\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def add_X0(self, X):\n",
    "        return np.column_stack((np.ones(X.shape[0]), X))\n",
    "\n",
    "    def normalize_testdata(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "        X = (X-self.mean) / self.std\n",
    "        X = self.add_X0(X)\n",
    "        return X\n",
    "\n",
    "    def normalize_traindata(self, X):\n",
    "        X = (X-self.mean) / self.std\n",
    "        X = self.add_X0(X)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #n = X.shape[0]\n",
    "        #y = np.where(y <= 0, -1, 1)\n",
    "        if self.normalize:\n",
    "          X = self.normalize_testdata(X)\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.b = 0\n",
    "        for i in range(0, self.max_iterations):\n",
    "            for idx, xi in enumerate(X):\n",
    "                condition = (y[0][idx] * (np.dot(self.w, xi))) + self.b\n",
    "                if condition >= 1:\n",
    "                    self.w = self.w - self.learning_rate * (2 * self.lambda_ * np.array(self.w))\n",
    "                else:\n",
    "\n",
    "                    self.w = self.w - self.learning_rate * ((2 * self.lambda_ * np.array(self.w))\n",
    "                                                            - np.dot(xi, y[0][idx]))\n",
    "                    self.b = self.b - (self.learning_rate * -y[0][idx])\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.normalize:\n",
    "          X = self.normalize_traindata(X)\n",
    "        pred = np.dot(X, self.w) + self.b\n",
    "        return pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WhpZ24chXPux",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Class to solve for multi class problem for SVM\n",
    "\n",
    "class multiclassSolver:\n",
    "\n",
    "    def __init__(self, X, y, model):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self):\n",
    "        self.weights_array = []\n",
    "        self.models_array = []\n",
    "        for i in np.unique(self.y):\n",
    "            print(\"Training for target Class\", i)\n",
    "            y_ = pd.DataFrame(np.where(self.y.copy() == i, 1, -1))\n",
    "            self.models_array.append(copy.copy(self.model))\n",
    "            weights = self.models_array[-1].fit(self.X, y_)\n",
    "            #weights = [sub_arr[0] for sub_arr in weights]\n",
    "            #self.weights_array.append(weights)\n",
    "        print(\"Training completed\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        for i in range (0, len(np.unique(self.y))):\n",
    "            pred.append(self.models_array[i].predict(X))\n",
    "        max_indices = np.argmax(pred, axis=0)\n",
    "        return max_indices\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S1dAIFjTXTZQ",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "model = SVM()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZDolycKaX7X8",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "a = multiclassSolver(trainX, trainY, model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cpMhLxgIX-RE",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "a.fit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "__hK_NDYYvo5",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "y_ = a.predict(testX)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "conf_matrix = confusion_matrix(testY, y_, 4)\n",
    "precision, recall, accuracy = precision_recall_accuracy(conf_matrix)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "id": "wbankyDqF0sV",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nhyy1aAzvryO"
   },
   "source": [
    "#PCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mWOqB_r9vuJa",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def pca_check(X):\n",
    "\n",
    "  flag=0\n",
    "  # Checking if all columns are numerical\n",
    "  if X.select_dtypes(include=np.number).equals(X):\n",
    "    print(\"All columns are numerical\")\n",
    "  else:\n",
    "    flag=1\n",
    "    print(\"PCA cannot be performed because all columns are not numerical.\")\n",
    "\n",
    "  # Checking dataset size\n",
    "  n, d = X.shape\n",
    "  if d <= n:\n",
    "    print(\"Dimensions are appropriate to perform PCA \")\n",
    "  else:\n",
    "    flag=1\n",
    "    print(\"Dimensions are not appropriate to perform PCA\")\n",
    "\n",
    "  # Checking linearity assumption using SVD\n",
    "\n",
    "  def svd(X):\n",
    "\n",
    "    XTX = np.dot(X.T, X)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(XTX)\n",
    "    sort_indices = np.argsort(eigenvalues)[::-1] # Sort in descending order\n",
    "    sort_eigenvectors = eigenvectors[:, sort_indices]\n",
    "    sort_eigenvalues = eigenvalues[sort_indices]\n",
    "    s = np.sqrt(sort_eigenvalues) # singular value\n",
    "    v = sort_eigenvectors # right singular vectors\n",
    "    u = np.dot(X, v) / s # left singular vectors\n",
    "    return u, s, v.T\n",
    "\n",
    "  mean_centering = X - X.mean(axis=0)\n",
    "  u, s, v = svd(X)\n",
    "  tolerance = max(X.shape) * np.finfo(X.values.dtype).eps # assumimg there is no noise-induced variations\n",
    "  min_s = np.min(s) #smallest singular value\n",
    "  if min_s < tolerance:\n",
    "  #if np.all(s > tolerance):\n",
    "    print(\"The relationships between variables in the dataset are not purely linear\")\n",
    "    flag=1\n",
    "  else:\n",
    "    print(\"The relationships between variables in the dataset are linear\")\n",
    "\n",
    "  if flag == 0:\n",
    "    print(\"All tests for PCA passed. PCA can be performed.\")\n",
    "  else:\n",
    "    print(\"All tests for PCA did not pass. PCA may still be performed, but results should be cautiously interpreted\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7nyDjXWPv49q",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "@dataclass\n",
    "class PCA:\n",
    "  X: None\n",
    "  mean: float = None\n",
    "  std: float = None\n",
    "  eigenvalues: None = None\n",
    "  eigenvectors: None = None\n",
    "  n_components: int = None\n",
    "\n",
    "  # Function to fit data and return principal components\n",
    "  def fit(self):\n",
    "    self.mean = np.mean(self.X, axis=0)\n",
    "    self.std = np.std(self.X, axis=0)\n",
    "    X_std = (self.X - self.mean) / self.std # Normalizing dataframe\n",
    "    covariance = np.cov(X_std, rowvar=False) # Calculating covariance matrix of normalized dataframe\n",
    "    self.eigenvalues, self.eigenvectors = np.linalg.eig(covariance)\n",
    "    sorted_indices = np.argsort(self.eigenvalues)[::-1]\n",
    "    self.eigenvalues = self.eigenvalues[sorted_indices]\n",
    "    self.eigenvectors = self.eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # Calculating total cumulative captured variance percentage for each column\n",
    "    total_var = np.sum(self.eigenvalues)\n",
    "    cumulative_var = np.cumsum(self.eigenvalues) / total_var\n",
    "\n",
    "    for i, var in enumerate(cumulative_var, 1):\n",
    "      percent = var * 100\n",
    "      print(f\"Cummulative variance captured by PC {i} is {percent:.2f}%\")\n",
    "\n",
    "  def transform(self, X, n_components):\n",
    "    top_eigenvectors = self.eigenvectors[:, :n_components]\n",
    "    X_std = (X - self.mean) / self.std\n",
    "    pca_X = np.dot(X_std, top_eigenvectors)\n",
    "    return pca_X\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o4g-HojXxnZG",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "pca_check(X_resampled)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lUiwGsdWwMks",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "pca = PCA(trainX)\n",
    "pca.fit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "15IQnJhdzFC7",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "trainX_pca = pca.transform(trainX,n_components=7)\n",
    "testX_pca = pca.transform(testX,n_components=7)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK8vftGezcjT"
   },
   "source": [
    "# SVM with PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Oim5r4ZAzo2D",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "model = SVM(normalize = False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XCE8HhWUzr9w",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "a = multiclassSolver(trainX_pca, trainY, model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z7RIvYSYz1MU",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "a.fit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vOPkBA1O0b1y",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "y_hat = a.predict(testX_pca)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "conf_matrix = confusion_matrix(testY, y_, 4)\n",
    "precision, recall, accuracy = precision_recall_accuracy(conf_matrix)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "id": "_l-bd_lOKyGL",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9uAHLfxUfpU"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jfnSmvAxLFfJ",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X_resampled, y_resampled, test_size = 0.20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w2zNQlawbTOk",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "trainY= np.array(trainY)\n",
    "mean = np.mean(trainX, axis=0)\n",
    "std = np.std(trainX, axis=0)\n",
    "trainX = (trainX - mean) / std\n",
    "trainX = np.array(trainX)\n",
    "trainX = np.column_stack((np.ones(shape=(trainX.shape[0])), trainX ))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JGUmtfrXhJUM",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "testX = (testX - mean) / std\n",
    "testX = np.array(testX)\n",
    "testX = np.column_stack((np.ones(shape=(testX.shape[0])), testX ))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DU-xBNOgfLHU",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression Class\n",
    "\n",
    "    Attributes\n",
    "    __________\n",
    "    iterations : number of iterations used for gradient descent to find optimal weights,\n",
    "    learning_rate : learning rate used for gradient descent(alpha),\n",
    "    tolerance : tolerance used for gradient descent convergence of difference in weights,\n",
    "    regularization_parameter : regularization parameter(lambda), Default = 0,\n",
    "    weights : weights of the model after fitting,\n",
    "    errors : errors of the model in each iteration of gradient descent\n",
    "\n",
    "    Methods\n",
    "    _______\n",
    "    fit(X_train, y_train) : train model on training set\n",
    "    predict(X_test) : predict the test data using fitted model weights,\n",
    "    evaluate(X_test,y_test) : predict the test data using fitted model weights and get evaluation results Accuracy,\n",
    "    Recall, F1 Score, Precision\n",
    "    \"\"\"\n",
    "    def __init__(self, iterations, learning_rate, tolerance):\n",
    "        self.errors = None\n",
    "        self.prev_cost = None\n",
    "        self.cost = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        self.y_train = None\n",
    "        self.weights = None\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        \"\"\"\n",
    "        Sigmoid function\n",
    "        :param x: input numpy array\n",
    "        :return: sigmoid(x)\n",
    "        \"\"\"\n",
    "        return np.abs(1 / (1 + np.exp(-x)))\n",
    "\n",
    "    def gradient_calculation(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the logistic regression model's loss function with respect to the model's weights.\n",
    "        The gradient calculation formula is:\n",
    "\n",
    "        ∇(loss) = X.T * (y_pred - y)\n",
    "        :return:\n",
    "        numpy.ndarray: A gradient vector where each element is the partial\n",
    "                       derivative of the loss function with respect to the\n",
    "                       corresponding weight.\n",
    "        \"\"\"\n",
    "        y_pred = self.sigmoid(np.matmul(self.X_train, self.weights))\n",
    "        return self.X_train.T.dot(y_pred - self.y_train)\n",
    "\n",
    "    def cost_function(self):\n",
    "        \"\"\"\n",
    "        Computes the cost for a logistic regression model using cross-entropy loss\n",
    "        and adds regularization penalty if applicable.\n",
    "\n",
    "        The cost function is defined as:\n",
    "\n",
    "            L = -sum(y * log(y_pred) + (1 - y) * log(1 - y_pred))\n",
    "\n",
    "        where:\n",
    "        - y is the vector of actual class labels,\n",
    "        - y_pred is the vector of predicted probabilities, computed as sigmoid(X * w),\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            float: The computed cost value.\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self.sigmoid(np.matmul(self.X_train, self.weights))\n",
    "        epsilon = 0.000001\n",
    "        cost = self.y_train * np.log(y_pred) + (1 - self.y_train) * np.log(1 - y_pred + epsilon) #Added epsilon because log 0 gives NaN and causes cost to be NaN\n",
    "        cost = -np.sum(cost)\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "        Performs gradient descent optimization to find the optimal weights of the logistic regression model.\n",
    "\n",
    "        The method iteratively updates the weights by moving in the direction of the negative gradient of the cost function, adjusted by the learning rate.\n",
    "\n",
    "        Weight update formulas:\n",
    "        - w = w - learning_rate * ∇(loss)\n",
    "\n",
    "        The process continues for a specified number of iterations or until the improvement in cost is less than a defined tolerance, indicating convergence.\n",
    "\n",
    "        Attributes updated during optimization:\n",
    "        - self.weights: The weights vector of the model.\n",
    "        - self.errors: A list of cost values at each iteration, tracking how the cost changes.\n",
    "\n",
    "        Side effects:\n",
    "        - If the change in cost between iterations is less than the tolerance, it prints a message and stops further updates.\n",
    "\n",
    "        \"\"\"\n",
    "        self.errors = []\n",
    "        self.weights = np.zeros(shape=(self.X_train.shape[1]))\n",
    "        self.prev_cost = np.inf\n",
    "        for i in range(self.iterations):\n",
    "            self.weights -= (self.learning_rate * self.gradient_calculation())\n",
    "            self.cost = self.cost_function()\n",
    "            self.errors.append(self.cost)\n",
    "            if self.prev_cost - self.cost <= self.tolerance:\n",
    "                print('Model has stopped improving')\n",
    "                break\n",
    "            self.prev_cost = self.cost\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fits the logistic regression model to the training data using gradient descent.\n",
    "\n",
    "        This method initializes the training process by setting the training data and target labels,\n",
    "        and then calls the `gradient_descent` method to optimize the model's weights.\n",
    "\n",
    "        Parameters:\n",
    "            X_train (array-like): The input features of the training data.\n",
    "            y_train (array-like): The target labels corresponding to the input features.\n",
    "\n",
    "        The training process involves:\n",
    "        - Storing the training data (`X_train`) and labels (`y_train`) in the instance variables.\n",
    "        - Calling the `gradient_descent` method to adjust the weights based on the loss gradient.\n",
    "\n",
    "        Note:\n",
    "        - The shape of `X_train` should match the expected number of features.\n",
    "        - The shape of `y_train` should correspond to the number of samples in `X_train`.\n",
    "        \"\"\"\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.gradient_descent()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predicts class labels for given input samples using the trained logistic regression model.\n",
    "\n",
    "        This method computes predictions by applying the sigmoid function to the linear combination\n",
    "        of input features and the learned weights. The output of the sigmoid function represents\n",
    "        the probability of the input belonging to the positive class, which is then threshold at 0.5\n",
    "        to produce binary class labels.\n",
    "\n",
    "        Parameters:\n",
    "            X_test (array-like): The input features of the test data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: An array of predicted class labels (0 or 1) for each input sample.\n",
    "\n",
    "        The prediction is calculated as follows:\n",
    "        - Apply the sigmoid function to (X_test * weights) to get the probability of the positive class.\n",
    "        - Threshold the probabilities at 0.5 to determine the class labels.\n",
    "        \"\"\"\n",
    "        predictions = self.sigmoid(np.matmul(X_test, self.weights))\n",
    "        return np.round(predictions)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluates the performance of the logistic regression model on a test dataset.\n",
    "\n",
    "        This method uses the model's predictions to calculate key classification metrics:\n",
    "        accuracy, precision, recall, and F1-score. These metrics provide insights into the\n",
    "        effectiveness of the model in classifying positive and negative classes.\n",
    "\n",
    "        Parameters:\n",
    "            X_test (array-like): The input features of the test data.\n",
    "            y_test (array-like): The actual class labels for the test data.\n",
    "\n",
    "        The evaluation metrics are calculated as follows:\n",
    "        - True Positives (TP): Correct positive predictions.\n",
    "        - False Positives (FP): Incorrect positive predictions.\n",
    "        - True Negatives (TN): Correct negative predictions.\n",
    "        - False Negatives (FN): Incorrect negative predictions.\n",
    "\n",
    "        Formulas:\n",
    "        - Accuracy = TP / (TP + FP)\n",
    "        - Recall = TP / (TP + FN)\n",
    "        - Precision = TP / (TP + FP)\n",
    "        - F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "        Side effects:\n",
    "        - Prints the calculated metrics: Accuracy, Recall, Precision, and F1-score.\n",
    "        \"\"\"\n",
    "        y_predicted = self.predict(X_test)\n",
    "        true_positives = np.sum(y_predicted * y_test)\n",
    "        false_positive = np.sum(y_predicted * (1 - y_test))\n",
    "        true_negative = np.sum((y_predicted == 0) & (y_test == 0))\n",
    "        false_negative = np.sum((y_predicted == 0) & (y_test == 1))\n",
    "\n",
    "        accuracy = true_positives / (true_positives + false_positive)\n",
    "        recall = true_positives / (true_positives + false_negative)\n",
    "        precision = true_positives / (true_positives + false_positive)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        print('Accuracy: ', accuracy)\n",
    "        print('Recall: ', recall)\n",
    "        print('Precision: ', precision)\n",
    "        print('F1-score: ', f1_score)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bi3EFQcBfLFW",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "class OnevAll():\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.y_test = None\n",
    "        self.X_test = None\n",
    "        self.y_temp = None\n",
    "        self.result_matrix = None\n",
    "        self.y_predictions = None\n",
    "        self.prob_df = None\n",
    "        self.unique_y = None\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.logistic_regression_weights = {}\n",
    "        self.softmax_values = {}\n",
    "        self.predicted_probs = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_softmax(x):\n",
    "        \"\"\"\n",
    "        Sigmoid function\n",
    "        :param x: input numpy array\n",
    "        :return: sigmoid(x)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def implement_logistic(self):\n",
    "        self.unique_y = np.unique(self.y)\n",
    "        for unique in self.unique_y:\n",
    "            self.y_temp = np.where(self.y.copy() == unique, 1, 0)\n",
    "            logistic_object =  LogisticRegression(iterations=50000, learning_rate=0.0001, tolerance=0.0001)\n",
    "            logistic_object.fit(self.X, self.y_temp)\n",
    "            self.logistic_regression_weights[unique] = logistic_object.weights\n",
    "            print(f\"Trained for Class {unique}\")\n",
    "            del logistic_object\n",
    "\n",
    "\n",
    "    def calculate_results(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        for key in self.logistic_regression_weights.keys():\n",
    "            self.softmax_values[key] = self.sigmoid_softmax(np.matmul(self.X_test,self.logistic_regression_weights[key]))\n",
    "\n",
    "        for key1 in self.softmax_values.keys():\n",
    "            denominator = np.zeros(self.softmax_values[key1].shape)\n",
    "            for key2 in self.softmax_values.keys():\n",
    "                if key1 == key2:\n",
    "                    numerator = self.softmax_values[key1]\n",
    "                denominator += self.softmax_values[key2]\n",
    "            self.predicted_probs[key1] = numerator / denominator\n",
    "\n",
    "        self.prob_df = pd.DataFrame.from_dict(self.predicted_probs)\n",
    "\n",
    "        self.y_predictions = np.zeros(self.y_test.shape, dtype=int)\n",
    "\n",
    "\n",
    "        for index, row in self.prob_df.iterrows():\n",
    "            # Find the column name of the maximum value in this row\n",
    "            max_col = row.idxmax()\n",
    "            # Append the column name to the list\n",
    "            self.y_predictions[index] = max_col\n",
    "\n",
    "        return self.y_predictions\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5N_Qmb3Vavhz",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "one_v_all = OnevAll(trainX, trainY)\n",
    "one_v_all.implement_logistic()\n",
    "y__ = one_v_all.calculate_results(testX,testY)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mGeEZ1eyavKd",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "conf_matrix = confusion_matrix(testY, y__, 4)\n",
    "precision, recall, accuracy = precision_recall_accuracy(conf_matrix)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yxrcDEJVQ3v"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "* SVM Accuracy improves after applying PCA.\n",
    "* Overall SVM is the best classifier."
   ],
   "metadata": {
    "id": "Efxfksi3O4Yl"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "q9pUJTlS8yqF"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "toc_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
